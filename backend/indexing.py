# 1. Initializes a Pinecone index for vector-based search.
# 2. Loads and processes CVE data, converting it into text embeddings using a SentenceTransformer model.
# 3. Stores these embeddings in the Pinecone index.
# 4. Performs a vector search based on a user query to find relevant CVE descriptions.
# 5. Uses the Groq API to generate a natural language response based on the search results.

import os
import torch
import json
from pinecone import Pinecone, ServerlessSpec
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer
from sentence_transformers import SentenceTransformer
from huggingface_hub.utils import insecure_hashlib 
from groq import Groq

load_dotenv()
pc = Pinecone(
        api_key=os.environ.get("PINECONE_API_KEY")
    )

# Create Index
index_name = "all-minilm-l12-v2"

if index_name not in pc.list_indexes().names():
    pc.create_index(
        name=index_name,
        dimension=384,
        metric="cosine",
        spec=ServerlessSpec(
            cloud='aws',
            region='us-east-1'
        )
    )

index = pc.Index(index_name)

device = 'cuda' if torch.cuda.is_available() else 'cpu'

model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2').to(device)

# Function to convert the loaded JSON data to the desired format
def convert_cve_json_to_embed_data(cve_json):
    embed_data = []
    
    for index, item in enumerate(cve_json):
        # cve_id = item['cveMetadata']['cveId']
        cve_id = item.get("cveMetadata", {}).get("cveId", "")
        description = item.get("containers", {}).get("cna", {}).get("descriptions", [{}])[0].get("value", "")
        
        embed_data.append({
            "id": cve_id,
            "text": description
        })
    
    return embed_data

# Load the JSON data from the file
with open('combined_cve_data.json', 'r') as file:
    cve_json = json.load(file)

# Convert the JSON data to the desired format
data = convert_cve_json_to_embed_data(cve_json)

#print(data)

sentences = [x['id']+x["text"] for x in data]
embeddings =  model.encode(sentences)  

vectors = []
for d, e in zip(data, embeddings):
    vectors.append({
        "id": d['id'],
        "values": e,
        "metadata": {'description': d['text'],
                     'id': d['id']}
    })

index.upsert(
    vectors=vectors,
    namespace="ns1"
)

query = "tell me some info about the Pro Macros provides XWiki rendering macros. Missing escaping in the Viewpdf macro allows any"

# query = "describe cve_id CVE-2024-6158"

# query = "some info about Cross-Site Request Forgery (CSRF) vulnerability?"

query_embedding = model.encode(query).tolist()
# print(query_embedding)

results = index.query(
    namespace="ns1",
    vector=query_embedding,
    top_k=1,
    include_values=False,
    include_metadata=True
)

print("results: ", results)

# Following is adding the retrieved information to the LLM system prompt, ask groqAPI to get the answer
matched_info = ' '.join(item['metadata']['description'] for item in results['matches'])
sources = [item['id'] for item in results['matches']]
context = f"Information: {matched_info} and the sources: {sources}"
sys_prompt = f"""
Instructions:
- Be helpful and answer questions concisely. If you don't know the answer, say 'I don't know'
- Utilize the context provided for accurate and specific information.
- Incorporate your preexisting knowledge to enhance the depth and relevance of your response.
- Cite your sources
Context: {context}
"""

client = Groq(
    api_key=os.environ.get("GROQ_API_KEY"),
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": sys_prompt,
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)
